{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e456354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "from scipy.signal import argrelmax\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Activation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.layers import Input, Conv2D,Conv1D,BatchNormalization, Multiply,Attention, MaxPooling2D, Bidirectional,  Flatten, Concatenate, Dot, Softmax, Dense, LSTM, Dropout, concatenate,BatchNormalization,Flatten, Reshape\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from scipy.signal import spectrogram\n",
    "from tensorflow.keras import initializers\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "import pickle\n",
    "from ctgan import CTGAN\n",
    "import xgboost as xgb\n",
    "from scipy import integrate\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.integrate import quad\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from math import cos, asin, sqrt, pi\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from scipy.fft import fft, fftfreq\n",
    "# importing the modules\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Reshape, Flatten, concatenate, Dot, Concatenate, Lambda\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "from matplotlib.mlab import specgram\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import interp1d\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_palette(\"husl\")\n",
    "import numpy as np\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dot, Concatenate, RepeatVector\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D,BatchNormalization, Attention, MaxPooling2D, Bidirectional,  Flatten, Concatenate, Dot, Softmax, Dense, LSTM, Dropout, concatenate,BatchNormalization,Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle as pkl\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0dcab-7488-4cc6-b148-cb78736b235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X train size:\",tab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615708f5-4a48-4f2c-95d8-93e1125b8211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MY MAIN MODEL\n",
    "from tensorflow.signal import fft\n",
    "\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    def create_attention_cnn_model(your_sequence_length, your_num_features):\n",
    "        input_layer = Input(shape=(your_sequence_length, your_num_features))\n",
    "        lstm_layer = LSTM(64,return_sequences=True)(input_layer)\n",
    "        lstm_layer = LSTM(128,return_sequences=True)(lstm_layer)\n",
    "        \n",
    "        flattened_representation = Flatten()(lstm_layer)\n",
    "        tabular_input = Input(shape=(16,))\n",
    "        concat = tf.keras.layers.concatenate([flattened_representation, tabular_input])\n",
    "        flatten_layer = Flatten()(concat)\n",
    "        dense1 = Dense(64, activation='relu')(flatten_layer)\n",
    "        dense1 = Dense(32, activation='relu')(dense1)\n",
    "        dense1 = Dense(16, activation='relu')(dense1)\n",
    "        output_layer = Dense(1)(dense1)\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=0.001,\n",
    "                decay_steps=100,\n",
    "                decay_rate=0.96,\n",
    "                staircase=True\n",
    "            )\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model_cnn_bilstm = Model(inputs=[input_layer, tabular_input], outputs=output_layer)\n",
    "        model_cnn_bilstm.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        return model_cnn_bilstm\n",
    "\n",
    "    your_sequence_length = 300 \n",
    "    your_num_features = 5\n",
    "    model = create_attention_cnn_model(your_sequence_length, your_num_features)\n",
    "    checkpoint = ModelCheckpoint(filepath='Attention_weights.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "    model.fit(x=[seismic_train,tab_train], y=y_train, batch_size=32, epochs=10, \n",
    "              validation_data=([seismic_val,tab_val], y_val),callbacks=[checkpoint,early_stopping])\n",
    "    model.load_weights('Attention_weights.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c94b96-5bc9-4360-b0e6-550dbba897d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([seismic_test,tab_test])\n",
    "y_pred_test=y_pred\n",
    "mae = mean_absolute_error(y_test,y_pred_test)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "count=0\n",
    "for i in range(0,y_test.shape[0]):\n",
    "    if np.abs(y_test[i])>=100:\n",
    "        if np.abs(y_pred[i])>=100:\n",
    "            count=count+1\n",
    "print(count)\n",
    "print(y_test.shape)\n",
    "plt.scatter(y_test,y_pred_test,s=10,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f827d9-6090-4b24-b23a-9467fa2b95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_ctgan(tab_train,y_pred_train,y_train):\n",
    "    combined_data = np.concatenate((tab_train,y_pred_train), axis=1)\n",
    "    \n",
    "    y_train_reshaped = tf.reshape(y_train, (-1, 1))\n",
    "    data = np.concatenate((combined_data,y_train_reshaped), axis=1)\n",
    "    print(data.shape)\n",
    "    ctgan = CTGAN(epochs=100)\n",
    "    print(\"Creating new data\")\n",
    "    ctgan.fit(data)\n",
    "    print(\"FITTED THE CTGAN mdoel\")\n",
    "    model_file = \"Model New/ctgan_Magnitude_Pred.pkl\"\n",
    "    with open(model_file, 'wb') as model_filename:\n",
    "        pickle.dump(ctgan, model_filename)\n",
    "    return ctgan\n",
    "\n",
    "model = load_model('Model New/Model_Magnitude_Predict.h5')\n",
    "\n",
    "y_pred_train = model.predict([seismic_train,tab_train])    \n",
    "ctgan_model_HIGHER=save_ctgan(tab_train,y_pred_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c9709-03ee-4d17-b2d6-dc8b862de02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = np.concatenate((tab_train,y_pred_train), axis=1)\n",
    "y_train_reshaped = tf.reshape(y_train, (-1, 1))\n",
    "data = np.concatenate((combined_data,y_train_reshaped), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb7651-5711-45e9-96a5-493dd551646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_output = model.predict([ seismic_test,tab_test]) \n",
    "combined_data = np.concatenate((tab_test, dense_output), axis=1)\n",
    "y_pred_train = model.predict([seismic_train,tab_train])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3beb1-9d0b-42be-9a61-f030247f540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data2 = np.concatenate((tab_train,y_pred_train), axis=1)\n",
    "y_train_reshaped = tf.reshape(y_train, (-1, 1))\n",
    "data = np.concatenate((combined_data2,y_train_reshaped), axis=1)\n",
    "model_file = \"Model New/ctgan_Magnitude_Pred.pkl\"\n",
    "with open(model_file, 'rb') as file:\n",
    "    ctgan_model_HIGHER = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b99941-6f41-4fde-8079-e091cca4dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data_high(synthetic_data,max_size):\n",
    "    selected_vectors=[]\n",
    "\n",
    "    sel6=(synthetic_data[np.round(synthetic_data[:, -1],1)==7.5])[:500]\n",
    "    sel7=(synthetic_data[np.round(synthetic_data[:, -1],1)==7.6])[:500]\n",
    "    sel8=(synthetic_data[np.round(synthetic_data[:, -1],1)==7.7])[:500]\n",
    "    sel9=(synthetic_data[np.round(synthetic_data[:, -1],1)==7.8])[:500]\n",
    "    sel10=(synthetic_data[np.round(synthetic_data[:, -1],1)==7.9])[:500]\n",
    "    sel11=(synthetic_data[np.round(synthetic_data[:, -1],1)==8.0])[:500]\n",
    "\n",
    "    data = np.concatenate((sel6,sel7), axis=0)\n",
    "    data = np.concatenate((data,sel8), axis=0)\n",
    "    data = np.concatenate((data,sel9), axis=0)\n",
    "    data = np.concatenate((data,sel10), axis=0)\n",
    "    data = np.concatenate((data,sel11), axis=0)\n",
    "    print(\"Synthetic dataset size\",data.shape)\n",
    "    return data\n",
    "\n",
    "    \n",
    "def call_ctgan(ctgan,flag,sample):\n",
    "    # Generate synthetic data\n",
    "    num_samples = sample  # Specify the number of synthetic samples you want to generate\n",
    "    synthetic_data = ctgan.sample(num_samples)\n",
    "\n",
    "    mask = (synthetic_data >= -1).all(axis=1)\n",
    "    # Apply the mask to filter the rows\n",
    "    filtered_synthetic_data = synthetic_data[mask]\n",
    "    filtered_synthetic_data[:, -1] = np.round(filtered_synthetic_data[:, -1], 1)\n",
    "\n",
    "    if flag==1:\n",
    "        synthetic_data=loading_data_high(filtered_synthetic_data,25)   \n",
    "    return synthetic_data\n",
    "    \n",
    "def train_ML(tab_train,y_pred_train,y_train,ctgan_model_HIGHER,data,combined_data):\n",
    "    synthetic_data=call_ctgan(ctgan_model_HIGHER,1,500000)\n",
    "    # synthetic_data = np.concatenate((syn_high,syn_low), axis=0)\n",
    "    print(\"Before Adding synthetic data\",data.shape)\n",
    "    new_data = np.concatenate((data,synthetic_data), axis=0)\n",
    "    df = pd.DataFrame(new_data)\n",
    "    print(\"After adding synthetic data\",new_data.shape)\n",
    "#     # Specify the CSV file path\n",
    "    csv_file = \"D:/Paper IJCAI/Output Dataset/Pred_Magnitude_Train_Synthetic_Mix_Data_2.csv\"\n",
    "    # Save the DataFrame to a CSV file without column names\n",
    "    df.to_csv(csv_file, index=False, header=False)\n",
    "    df_1 = pd.DataFrame(synthetic_data)\n",
    "    print(\"After adding synthetic data\",data.shape)\n",
    "#     # Specify the CSV file path\n",
    "    csv_file = \"D:/Paper IJCAI/Output Dataset/Pred_Magnitude_Train_Synthetic_Data.csv\"\n",
    "    # Save the DataFrame to a CSV file without column names\n",
    "    df_1.to_csv(csv_file, index=False, header=False)\n",
    "    \n",
    "    new_data=pd.read_csv(csv_file,sep=',')\n",
    "    dataset=new_data.values\n",
    "    t=dataset.shape[1]-1\n",
    "    X_train_new = dataset[:,0:t]\n",
    "    y_train_new = dataset[:,t]\n",
    "\n",
    "train_ML(tab_train,y_pred_train,y_train,ctgan_model_HIGHER,data,combined_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d70de-8565-4171-9ac1-517219d0c680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_error(pga_test,predictions):\n",
    "    predictions=np.round(predictions,decimals=2)\n",
    "    print(\"Size is: \",predictions.shape)\n",
    "    mae = mean_absolute_error(pga_test,predictions)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    diff=np.abs(pga_test- predictions)\n",
    "    std_dev_pga = np.std(diff)\n",
    "    print(\"Standard deviation is\",std_dev_pga)\n",
    "    rmse = np.sqrt(mean_squared_error(pga_test, predictions))\n",
    "    pred_error=pga_test-predictions\n",
    "    print(\"Root Mean Squared Error:\", rmse)\n",
    "    plt.scatter(pga_test,predictions,s=10,color='black') \n",
    "\n",
    "def evaluate_error_channel(pga_test, waveform_test, tabular_test,model,\n",
    "                           LGBM_model,combined_data):\n",
    "    print(\"Shape\",combined_data.shape)\n",
    "    predictions=LGBM_model.predict(combined_data)\n",
    "    predictions=np.round(predictions,decimals=2)\n",
    "    print(\"Size is: \",predictions.shape)\n",
    "    print(\"y_test\",pga_test.shape)\n",
    "    mae = mean_absolute_error(pga_test,predictions)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    diff=np.abs(pga_test- predictions)\n",
    "    std_dev_pga = np.std(diff)\n",
    "    print(\"Standard deviation is\",std_dev_pga)\n",
    "    rmse = np.sqrt(mean_squared_error(pga_test, predictions))\n",
    "    pred_error=pga_test-predictions\n",
    "    print(\"Root Mean Squared Error:\", rmse)\n",
    "    plt.scatter(pga_test,predictions,s=10,color='black')\n",
    "    plt.xlim(2, 500)\n",
    "    plt.ylim(2, 500)    \n",
    "    return predictions\n",
    "    \n",
    "def Model_ML(tab_train,y_pred_train,y_train,y_test,seismic_test,tab_test,combined_data,dense_output):   \n",
    "    csv_file = \"D:/Paper IJCAI/Output Dataset/Pred_Magnitude_Train_Synthetic_Mix_Data_2.csv\"\n",
    "    new_data=pd.read_csv(csv_file,sep=',')\n",
    "    dataset=new_data.values\n",
    "    t=dataset.shape[1]-1\n",
    "    X_train_new = dataset[:,0:t]\n",
    "    y_train_new = dataset[:,t]\n",
    "    print(X_train_new.shape)\n",
    "    XGB_model = xgb.XGBRegressor(max_depth=2,learning_rate=0.1,n_estimators=300,subsample=0.6,seed=1,\n",
    "                            alpha=0.1,min_child_weight=1,random_state=1,scale_pos_weight = 1,base_score=10,\n",
    "                            reg_lambda=0.2 )\n",
    "    CatB_model = CatBoostRegressor(iterations=200,  # Number of trees (boosting rounds)\n",
    "                         learning_rate=0.01,  # Step size for the optimization\n",
    "                        depth=6,  # Tree depth\n",
    "                        subsample=0.7,\n",
    "                         verbose=100)\n",
    "    RF1_model = RandomForestRegressor(n_estimators=200,  # Number of trees in the forest\n",
    "                             max_depth=3,  # Maximum depth of each tree\n",
    "                             random_state=42)  # Set a random seed for reproducibility  \n",
    "    LGBM_model = lgb.LGBMRegressor(n_estimators=200,learning_rate=0.15,max_depth=6,random_state=42)  # Set a random seed for reproducibility\n",
    "    XGB_model_Real = xgb.XGBRegressor(max_depth=4,learning_rate=0.1,n_estimators=100,subsample=0.6,seed=1,\n",
    "                            alpha=0.1,min_child_weight=1,random_state=1,scale_pos_weight = 1,base_score=10,\n",
    "                            reg_lambda=0.2 )\n",
    "    CatB_model_Real = CatBoostRegressor(iterations=100,  # Number of trees (boosting rounds)\n",
    "                         learning_rate=0.1,  # Step size for the optimization\n",
    "                        depth=6,  # Tree depth\n",
    "                        subsample=0.7,\n",
    "                         verbose=100)\n",
    "    RF1_model_real = RandomForestRegressor(n_estimators=100,  # Number of trees in the forest\n",
    "                             max_depth=10,  # Maximum depth of each tree\n",
    "                             random_state=42)  # Set a random seed for reproducibility  \n",
    "    LGBM_model_Real = lgb.LGBMRegressor(n_estimators=200,learning_rate=0.1,max_depth=10,random_state=42)  # Set a random seed for reproducibility\n",
    "    dense_output_train = model.predict([ seismic_train,tab_train]) \n",
    "    combined_train = np.concatenate((tab_train, dense_output_train), axis=1)\n",
    "    X_train=combined_train\n",
    "    XGB_model_Real.fit(X_train, y_train)\n",
    "    XGB_model.fit(X_train_new, y_train_new)\n",
    "    predictions_XGB=evaluate_error_channel(y_test, seismic_test,tab_test,model,XGB_model,combined_data)\n",
    "    predictions_XGB_Real=evaluate_error_channel(y_test, seismic_test,tab_test,model,XGB_model_Real,combined_data)\n",
    "    y_train = np.array(y_train)\n",
    "    X_train = np.array(X_train)\n",
    "    print(dense_output.shape)\n",
    "    dense_output=dense_output.flatten()\n",
    "    pred=(predictions_XGB_Real+predictions_XGB)/2\n",
    "\n",
    "    print(\"Final is\",np.mean(pred))\n",
    "    plt.scatter(y_test,pred,s=10,color='black')\n",
    "    plt.xlim(2.5, 9)\n",
    "    plt.ylim(2.5, 9)    \n",
    "    pred=np.abs(pred)\n",
    "    evaluate_error(y_test,pred)\n",
    "    file_path ='Model New/Pred_Mag_XGB_main_model_AUG.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(XGB_model, file)\n",
    "    file_path ='Model New/Pred_Mag_CatB_main_model_AUG.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(CatB_model, file)\n",
    "    file_path ='Model New/Pred_Mag_RF1_main_model_AUG.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(RF1_model, file)\n",
    "    file_path ='Model New/Pred_Mag_LGBM_main_model_AUG.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(LGBM_model, file)\n",
    "\n",
    "    file_path ='Model New/Pred_Mag_XGB_main_model_REAL.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(XGB_model_Real, file)\n",
    "    file_path ='Model New/Pred_Mag_CatB_main_model_REAL.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(CatB_model_Real, file)\n",
    "    file_path ='Model New/Pred_Mag_RF1_main_model_REAL.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(RF1_model_real, file)\n",
    "    file_path ='Model New/Pred_Mag_LGBM_main_model_REAL.pkl'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(LGBM_model_Real, file)\n",
    "        \n",
    "    return XGB_model,CatB_model,RF1_model,LGBM_model\n",
    "print(\"1. CNNAttention-----------\")\n",
    "print(\"TEST\")\n",
    "\n",
    "y_pred_latest = model.predict([seismic_latest,tab_latest])   \n",
    "dense_output = model.predict([ seismic_latest,tab_latest]) \n",
    "combined_data = np.concatenate((tab_latest, dense_output), axis=1)\n",
    "XGB_model,CatB_model,RF1_model,LGBM_model=Model_ML(tab_train,y_pred_train,y_train,y_latest,seismic_latest,tab_latest,combined_data,dense_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
